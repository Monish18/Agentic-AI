{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ab4329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3990307",
   "metadata": {},
   "source": [
    "Build a chain that takes an article or paragraph and returns a structured JSON with:\n",
    "\n",
    "title: a short title for the paragraph\n",
    "\n",
    "summary: 2-sentence summary\n",
    "\n",
    "keywords: list of 3 keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49293393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.fix import OutputFixingParser\n",
    "from langchain_groq import ChatGroq\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5a2b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parser data model\n",
    "\n",
    "class Summary(pydantic.BaseModel):\n",
    "\n",
    "    title: str\n",
    "    summary: str\n",
    "    keywords: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61db090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output parser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d888e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chat promt template\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI that extracts title, summary and keywords. {format_instructions}\"),\n",
    "        (\"user\", \"{input}\")\n",
    "        ]\n",
    "    ).partial(format_instructions=parser.get_format_instructions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eb86c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "\n",
    "llm = ChatGroq(model='qwen-qwq-32b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28136095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output fix parser\n",
    "\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect everything in a chain\n",
    "chain = prompt | llm | fixing_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9630cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "üëã Hi, I‚Äôm Monish Bangera ‚Äî a data engineer passionate about building intelligent, scalable systems that drive real-world impact.\n",
    "üöÄ I specialize in full-stack data engineering and cloud-native architectures. From designing lakehouse systems on AWS + Snowflake to shipping production-grade APIs, I bring ideas to life with clean, tested, production-ready code.\n",
    "üß† Currently diving deep into LangChain, agentic AI, and orchestration frameworks ‚Äî on the path to mastering AI and building tools to help developers harness modern AI workflows.\n",
    "üìú I‚Äôm AWS Cloud Practitioner and Terraform certified, preparing for the AWS Solutions Architect Associate (SAA) and AWS AI Certification to level up my cloud and AI expertise.\n",
    "üõ†Ô∏è Tech I love: Python, Pydantic, Airflow, AWS (S3, Lambda, SNS, MWAA), Snowflake, MongoDB, Terraform, LangChain, Dash, Flask, SQL, GitLab CI/CD\n",
    "üíº Currently a senior data engineer, leading projects in credit evaluation, billing automation, and NLP pipelines ‚Äî used by 30+ analysts across teams.\n",
    "üìà I share my journey, projects, and learnings via content and collaborations ‚Äî always open to connect and grow in AI + data.\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"input\": input_text,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef0d2fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Monish Bangera | Senior Data Engineer & Cloud Architect' summary='Monish Bangera is a senior data engineer with expertise in building intelligent, scalable systems through full-stack data engineering and cloud-native architectures. Specializing in AWS and Snowflake lakehouse systems, he designs production-grade APIs and NLP pipelines while focusing on modern AI workflows with LangChain and orchestration frameworks. Certified in AWS and Terraform, he leads projects in credit evaluation, billing automation, and cloud infrastructure, leveraging Python, Airflow, and DevOps tools. Actively pursuing AWS Solutions Architect and AI certifications, Monish shares his technical journey through content and collaborations.' keywords=['Data Engineering', 'Cloud-Native Architectures', 'AWS', 'Snowflake', 'LangChain', 'AI Workflows', 'AWS Cloud Practitioner', 'Terraform Certified', 'AWS SAA', 'AWS AI Certification', 'Python', 'Pydantic', 'Apache Airflow', 'S3', 'Lambda', 'SNS', 'MWAA', 'MongoDB', 'Dash', 'Flask', 'SQL', 'GitLab CI/CD', 'Production-Grade APIs', 'NLP Pipelines', 'Credit Evaluation', 'Billing Automation', 'Agentic AI', 'Orchestration Frameworks']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce213f",
   "metadata": {},
   "source": [
    "## ‚úÖ Exercise 2\n",
    "**Task:** List 5 skills required for an AI Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29f72539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic class for skills\n",
    "\n",
    "class Ai(pydantic.BaseModel):\n",
    "\n",
    "    skills: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9bf227cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Ai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9676a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"List 5 skills required for a AI Engineer. {formated_instructions}\"),\n",
    "        (\"user\", \"Please list them\")\n",
    "        ]\n",
    "    ).partial(formated_instructions=parser.get_format_instructions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e2da4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixing_parser = OutputFixingParser.from_llm(llm=llm, parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c288e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | fixing_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ebb30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd37f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skills=['Proficiency in programming languages such as Python', 'Strong understanding of machine learning and deep learning algorithms', 'Data preprocessing and analysis skills', 'Experience with AI frameworks (e.g., TensorFlow, PyTorch)', 'Ability to design and implement end-to-end AI solutions']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
